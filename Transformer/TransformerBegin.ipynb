{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848a7bb83704e849",
   "metadata": {},
   "source": [
    "# What does the pipeline do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T20:53:03.255708Z",
     "start_time": "2024-08-12T20:53:01.565038Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2c95d7ae7e6c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9a68d46648f87ec",
   "metadata": {},
   "source": [
    "Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb163754e4c375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:06:22.080773Z",
     "start_time": "2024-08-10T21:06:21.736574Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651dd7850eba966",
   "metadata": {},
   "source": [
    "Why Can tokenizer Accept Arguments?\n",
    "`tokenizer` as a Callable Object:\n",
    "In Python, classes can define a special method called __call__. If a class has this method, instances of the class can be called like a function.\n",
    "The AutoTokenizer (or more specifically, the tokenizer class that AutoTokenizer.from_pretrained returns) has a `__call__` method defined, which allows you to pass arguments directly to it as if it were a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381163262ea0ea3a",
   "metadata": {},
   "source": [
    "The following model contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. \n",
    "\n",
    "For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "1. Batch size: The **number of sequences** processed at a time (2 in our example).\n",
    "\n",
    "2. Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "\n",
    "3. Hidden size: The vector dimension of each model input. (768 in our example). The hidden_size refers to the dimensionality (i.e., the number of features or components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53697c2c4f7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "print(type(outputs['last_hidden_state']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea18e90245748c",
   "metadata": {},
   "source": [
    "The following model has a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the AutoModel class, but AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa70610cc9cc26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:35:21.483192Z",
     "start_time": "2024-08-10T23:35:20.631144Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26629b20c5b1ae88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:37:30.826142Z",
     "start_time": "2024-08-10T23:37:30.757212Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77b308258973cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:41:32.828245Z",
     "start_time": "2024-08-10T23:41:32.799150Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da874a58e7c7a",
   "metadata": {},
   "source": [
    " the model predicted [0.0402 Negative, 0.9598 Positive] for the first sentence and [0.9995, 0.0005] for the second one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b393927a64fff7",
   "metadata": {},
   "source": [
    "# Model\n",
    "## AutoModel\n",
    "- AutoModel class, which is handy when you want to instantiate any model from a checkpoint.\n",
    "- wrappers over model library. \n",
    "- automatically guess the appropriate model architecture for your checkpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c61005e125b13d",
   "metadata": {},
   "source": [
    "Randomly initializing Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac4e6839879f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:44:42.607215Z",
     "start_time": "2024-08-11T00:44:40.947254Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8be241a79ba33",
   "metadata": {},
   "source": [
    "Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ae1e7c579a68a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T20:53:32.439801Z",
     "start_time": "2024-08-12T20:53:32.013324Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8851eb9a82f568c",
   "metadata": {},
   "source": [
    "saves two files\n",
    "\n",
    "config.json: architecture\n",
    "\n",
    "pytorch_model.bin: weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20856458e0f5ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:53:12.366038Z",
     "start_time": "2024-08-12T21:53:11.149337Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pathlib\n",
    "# cur_path = pathlib.Path(__file__).parent.resolve()\n",
    "# import inspect\n",
    "# cur_path = inspect.getfile(lambda: None) #The code inspect.getfile(lambda: None) returns the path to the file where the lambda function is defined. In a Jupyter Notebook, this will not return the directory of the notebook itself. Instead, it will return a path related to the Jupyter environment, which is not useful for saving files relative to the notebook.\n",
    "# # model.save_pretrained(cur_path)\n",
    "import os\n",
    "cur_path = os.getcwd()\n",
    "print(cur_path)\n",
    "model.save_pretrained(cur_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad66f4a47872ad",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a84b662bdc43e",
   "metadata": {},
   "source": [
    "Loading the BERT tokenizer trained with the same checkpoint as BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82907eab3f61df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:22:05.911862Z",
     "start_time": "2024-08-12T22:22:03.628979Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d5f2d12e6847",
   "metadata": {},
   "source": [
    "AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74005db4fccdac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96aac43a084f6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:24:28.641863Z",
     "start_time": "2024-08-12T22:24:28.615614Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ffab66fd8bb86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:25:24.080400Z",
     "start_time": "2024-08-12T22:25:24.043098Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(cur_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaffe87bad73f60",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50922ab34b7fdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:34:59.526461Z",
     "start_time": "2024-08-12T22:34:49.429585Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492bf7197f75746",
   "metadata": {},
   "source": [
    "Encoding: from tokenizaer to input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fa92c82b06ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:37:25.348665Z",
     "start_time": "2024-08-12T22:37:25.325791Z"
    }
   },
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fbc2bdf05ac70",
   "metadata": {},
   "source": [
    "Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da297d9b834a717",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:38:27.615193Z",
     "start_time": "2024-08-12T22:38:27.580864Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aafce6a713ac69f",
   "metadata": {},
   "source": [
    "# Handling Muptiple Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71799a0196fda0b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:02:25.208075Z",
     "start_time": "2024-08-13T00:02:23.747069Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f9f0347216c85",
   "metadata": {},
   "source": [
    "Why IndexError: too many indices for tensor of dimension 1?\n",
    "the tokenizer didn’t just convert the list of input IDs into a tensor, it added a dimension on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e2b498272a4df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:08:54.231343Z",
     "start_time": "2024-08-13T00:08:54.162042Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bb5f37202e409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:08:59.109643Z",
     "start_time": "2024-08-14T17:08:58.128719Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)  # ['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])  #add a dimension\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8100af431617ec6",
   "metadata": {},
   "source": [
    "**Batching** is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c585a86240b28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:16:59.726992Z",
     "start_time": "2024-08-13T00:16:59.646671Z"
    }
   },
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]\n",
    "batched_ids=torch.tensor(batched_ids)\n",
    "output = model(batched_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef4c1b221f4484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:41:29.256753Z",
     "start_time": "2024-08-13T00:41:28.348484Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61b70bc22441b6",
   "metadata": {},
   "source": [
    "Why does the second sentence output inconsistency?\n",
    "Use attention mask to ignore the padding tokens.\n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8897514b1628010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T01:43:17.499510Z",
     "start_time": "2024-08-13T01:43:16.943288Z"
    }
   },
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b3b5f4a3d76d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:43:31.226723Z",
     "start_time": "2024-08-14T17:43:31.096258Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence3=\"I hate this so much!\"\n",
    "sequence4=\"I have been waiting for a HuggingFace course my whole life.\"\n",
    "sequence3_tokens = tokenizer.tokenize(sequence3)\n",
    "sequence4_tokens = tokenizer.tokenize(sequence4)\n",
    "sequence3_ids = tokenizer.convert_tokens_to_ids(sequence3_tokens)\n",
    "print(\"Length of sequence3_ids is:\",len(sequence3_ids))\n",
    "sequence4_ids = tokenizer.convert_tokens_to_ids(sequence4_tokens)\n",
    "print(\"Length of sequence4_ids is:\",len(sequence4_ids))\n",
    "sequence3_output = model(torch.tensor([sequence3_ids]))\n",
    "print(\"sequence3's Logits:\", sequence3_output.logits)\n",
    "sequence4_output=model(torch.tensor([sequence4_ids]))\n",
    "print(\"sequence4's Logits:\", sequence4_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d43f3de21406a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:48:54.863451Z",
     "start_time": "2024-08-14T17:48:54.579794Z"
    }
   },
   "outputs": [],
   "source": [
    "#padding sequence3_ids to 13\n",
    "print(\"length of sequence3_ids\", len(sequence3_ids))\n",
    "padding_sequence3_ids = sequence3_ids + [tokenizer.pad_token_id]*(len(sequence4_ids)-len(sequence3_ids))\n",
    "batch34_ids=torch.tensor([padding_sequence3_ids, sequence4_ids]) \n",
    "batch34_output_noAttentionMask=model(batch34_ids)\n",
    "print(\"batch34_output_noAttentionMask's logits are\",batch34_output_noAttentionMask.logits)\n",
    "# attention_mask\n",
    "attention_mask = [\n",
    "    [1]*len(sequence3_ids) + [0]*(len(sequence4_ids)-len(sequence3_ids)),  # [1]*13 + [0]*(13-13) => [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    [1]*len(sequence4_ids)  # [1]*13 => [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "print(\"attention mask is: \",attention_mask)\n",
    "batch34_output_withAttentionMask=model(batch34_ids, attention_mask=torch.tensor(attention_mask))\n",
    "print(\"batch34_output_withAttentionMask's logits are\",batch34_output_withAttentionMask.logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e32e5b9502a6f",
   "metadata": {},
   "source": [
    "Longer sequences\n",
    "\n",
    "Try other models that can handle longer sequences\n",
    " Longformer, LED\n",
    "\n",
    "or truncate the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309d6836c188e68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7ee32dde4bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = sequence[:max_sequence_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde6173ac7fd4e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea54273210d5c4e",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899b6103c7f9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965c995e170ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sequences \n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d240df9e5ee70c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncate the sequences\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc0f02f7d0a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensors\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7afcbcd33470c",
   "metadata": {},
   "source": [
    "## Special tokens\n",
    "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don’t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8153a48fb01d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T18:34:21.092194Z",
     "start_time": "2024-08-14T18:34:21.075953Z"
    }
   },
   "outputs": [],
   "source": [
    "#special tokens\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "#decode\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ad2cb28372fdc",
   "metadata": {},
   "source": [
    "# Wrapping up: From tokenizer to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531e663bd7ab4d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T18:43:52.290326Z",
     "start_time": "2024-08-14T18:43:51.964548Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72d6fcc296cfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

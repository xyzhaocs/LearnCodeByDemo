{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机  [nn.Linear()](https://www.cnblogs.com/wupiao/articles/13288616.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.weight.shape:\n",
      "  torch.Size([4, 3])\n",
      "m.bias.shape:\n",
      " torch.Size([4])\n",
      "output.shape:\n",
      " torch.Size([10, 4])\n",
      "ans.shape:\n",
      " torch.Size([10, 4])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(10, 3)  # 输入的维度是（128，20）\n",
    "m = torch.nn.Linear(3, 4)  # 20,30是指维度\n",
    "output = m(x)\n",
    "print('m.weight.shape:\\n ', m.weight.shape)\n",
    "print('m.bias.shape:\\n', m.bias.shape)\n",
    "print('output.shape:\\n', output.shape)\n",
    "\n",
    "# ans = torch.mm(input,torch.t(m.weight))+m.bias 等价于下面的\n",
    "ans = torch.mm(x, m.weight.t()) + m.bias #torch.mm(a, b)是矩阵a和b矩阵相乘\n",
    "print('ans.shape:\\n', ans.shape)\n",
    "\n",
    "print(torch.equal(ans, output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5675,  0.3779,  0.1213],\n",
      "        [ 0.1757,  0.3552,  0.5695],\n",
      "        [ 0.1730, -0.4120, -0.3574],\n",
      "        [-0.0985, -0.4828, -0.3555]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3839,  0.0083,  0.4317, -0.2801], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(m.weight)\n",
    "print(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=torch.mm(x,m.weight.t())+m.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2=m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a1=[1,2,3,4]\n",
    "a2=np.array(a1)\n",
    "print(type(a1))\n",
    "print(type(a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tensor在cpu上才能转numpy* \n",
    "```\n",
    "for name, para in model.named_parameters():\n",
    "    if \"ssf_scale\" in name:\n",
    "        ssf_scale[name]=para.data.detach().cpu().numpy()\n",
    "    elif \"ssf_shift\" in name:\n",
    "        ssf_shift[name]=para.data.detach().cpu().numpy()\n",
    "    else:\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb= (c1 == c2)\n",
    "bb.flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([10, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1982,  1.5363,  0.0549, -0.9635, -1.2077, -0.1312,  0.3601,  0.2268,\n",
       "          0.0562, -0.4309],\n",
       "        [-0.8356,  0.3603,  0.8435, -0.5880,  0.2928,  1.7503,  1.2666, -0.2376,\n",
       "         -0.4501,  1.4948],\n",
       "        [-0.5694,  1.1981,  0.0665, -1.4922,  1.1955, -0.7883,  0.1208,  0.4790,\n",
       "         -0.3644, -1.7003]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 3)\n",
    "print(x.shape)\n",
    "print(x.t().shape)\n",
    "print(x.shape)\n",
    "x.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch的[Hook](https://blog.csdn.net/weixin_44878336/article/details/133859089)函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward前\n",
      "tensor(2.)\n",
      "backward后\n",
      "\n",
      "x.requires_grad: True\n",
      "y.requires_grad: True\n",
      "z.requires_grad: True\n",
      "\n",
      "x.grad: tensor([-4., -4.])\n",
      "y.grad: tensor([4., 4.])\n",
      "z.grad: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def hook_fn(grad):\n",
    "    grad *= 2\n",
    "    print(grad)\n",
    "    return grad\n",
    "\n",
    "x = torch.Tensor([1, 2]).requires_grad_()\n",
    "y = torch.Tensor([3, 4]).requires_grad_()\n",
    "z = ((y-x) ** 2).mean()\n",
    "\n",
    "z.register_hook(hook_fn)\n",
    "# z.register_hook(lambda x: 2*x)\n",
    "\n",
    "print(\"backward前\")\n",
    "z.backward()\n",
    "print(\"backward后\\n\")\n",
    "\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"z.requires_grad: {z.requires_grad}\\n\")\n",
    "\n",
    "print(f\"x.grad: {x.grad}\")\n",
    "print(f\"y.grad: {y.grad}\")\n",
    "print(f\"z.grad: {z.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.backward of tensor(4., grad_fn=<MeanBackward0>)>\n"
     ]
    }
   ],
   "source": [
    "z=((y-x)**2).mean()\n",
    "print(z.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "代码来自 : https://github.com/wzlxjtu/PositionalEncoding2D/blob/master/positionalembedding2d.py\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def positional_encoding(d_model, length):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the token\n",
    "    :param length: (maximum) token number\n",
    "    :return: length*d_model position matrix\n",
    "    \"\"\"\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(length, d_model)\n",
    "    position = torch.arange(0, length).unsqueeze(1)\n",
    "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                         -(math.log(10000.0) / d_model)))\n",
    "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "    return pe\n",
    "\n",
    "\n",
    "pe = positional_encoding(128, 10)\n",
    "plt.plot(range(10), pe[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [彻底理解 Pytorch 中的 squeeze() 和 unsqueeze()函数](https://zhuanlan.zhihu.com/p/368920094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 1])\n",
      "torch.Size([2, 1, 3, 1])\n",
      "torch.Size([2, 1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "a=torch.arange(0,6).view(2,1,3,1)\n",
    "print(a.shape)\n",
    "b=a.squeeze(0)\n",
    "print(a.shape)\n",
    "print(b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DropPath](https://blog.csdn.net/qq_43426908/article/details/121662843)用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4926, -0.9163, -2.8298,  1.8312, -0.5280],\n",
      "        [ 0.8809,  0.2158, -0.3715, -1.8313, -0.5842]])\n",
      "(2, 1)\n",
      "tensor([[0.6601],\n",
      "        [1.2221]])\n",
      "tensor([[0.],\n",
      "        [1.]])\n",
      "tensor([[-0.8209, -1.5272, -4.7163,  3.0519, -0.8800],\n",
      "        [ 1.4682,  0.3596, -0.6192, -3.0521, -0.9737]]) tensor([[0.],\n",
      "        [1.]])\n",
      "tensor([[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "        [ 1.4682,  0.3596, -0.6192, -3.0521, -0.9737]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1) \n",
    "    print(shape) \n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    print(random_tensor)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    print(random_tensor)\n",
    "    print(x.div(keep_prob),random_tensor)\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "# class DropPath(nn.Module):\n",
    "#     def __init__(self, drop_prob=None):\n",
    "#         super(DropPath, self).__init__()\n",
    "#         self.drop_prob = drop_prob\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "a=torch.randn(2,5)\n",
    "print(a)\n",
    "print(drop_path(a,0.4,True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1584]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((1,1), dtype=x.dtype, device=x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print((1,)+(1,)+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.2500,  2.5000,  3.7500,  5.0000],\n",
       "        [ 6.2500,  7.5000,  8.7500, 10.0000, 11.2500]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,10).view(2,5)\n",
    "print(x)\n",
    "x.div(0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

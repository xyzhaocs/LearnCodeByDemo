{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e208f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "d = envs.registry\n",
    "for k, v in d.items():\n",
    "    print(f\"{k} -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb49d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 28.0, Epsilon: 0.99\n",
      "Episode 1, Reward: 18.0, Epsilon: 0.99\n",
      "Episode 2, Reward: 19.0, Epsilon: 0.99\n",
      "Episode 3, Reward: 31.0, Epsilon: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\35320\\AppData\\Local\\Temp\\ipykernel_16208\\2259826469.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4, Reward: 16.0, Epsilon: 0.98\n",
      "Episode 5, Reward: 28.0, Epsilon: 0.97\n",
      "Episode 6, Reward: 30.0, Epsilon: 0.97\n",
      "Episode 7, Reward: 15.0, Epsilon: 0.96\n",
      "Episode 8, Reward: 20.0, Epsilon: 0.96\n",
      "Episode 9, Reward: 16.0, Epsilon: 0.95\n",
      "Episode 10, Reward: 11.0, Epsilon: 0.95\n",
      "Episode 11, Reward: 14.0, Epsilon: 0.94\n",
      "Episode 12, Reward: 15.0, Epsilon: 0.94\n",
      "Episode 13, Reward: 26.0, Epsilon: 0.93\n",
      "Episode 14, Reward: 27.0, Epsilon: 0.93\n",
      "Episode 15, Reward: 35.0, Epsilon: 0.92\n",
      "Episode 16, Reward: 24.0, Epsilon: 0.92\n",
      "Episode 17, Reward: 61.0, Epsilon: 0.91\n",
      "Episode 18, Reward: 18.0, Epsilon: 0.91\n",
      "Episode 19, Reward: 20.0, Epsilon: 0.90\n",
      "Episode 20, Reward: 12.0, Epsilon: 0.90\n",
      "Episode 21, Reward: 28.0, Epsilon: 0.90\n",
      "Episode 22, Reward: 17.0, Epsilon: 0.89\n",
      "Episode 23, Reward: 18.0, Epsilon: 0.89\n",
      "Episode 24, Reward: 21.0, Epsilon: 0.88\n",
      "Episode 25, Reward: 16.0, Epsilon: 0.88\n",
      "Episode 26, Reward: 21.0, Epsilon: 0.87\n",
      "Episode 27, Reward: 22.0, Epsilon: 0.87\n",
      "Episode 28, Reward: 37.0, Epsilon: 0.86\n",
      "Episode 29, Reward: 18.0, Epsilon: 0.86\n",
      "Episode 30, Reward: 38.0, Epsilon: 0.86\n",
      "Episode 31, Reward: 28.0, Epsilon: 0.85\n",
      "Episode 32, Reward: 25.0, Epsilon: 0.85\n",
      "Episode 33, Reward: 59.0, Epsilon: 0.84\n",
      "Episode 34, Reward: 34.0, Epsilon: 0.84\n",
      "Episode 35, Reward: 34.0, Epsilon: 0.83\n",
      "Episode 36, Reward: 131.0, Epsilon: 0.83\n",
      "Episode 37, Reward: 33.0, Epsilon: 0.83\n",
      "Episode 38, Reward: 118.0, Epsilon: 0.82\n",
      "Episode 39, Reward: 24.0, Epsilon: 0.82\n",
      "Episode 40, Reward: 22.0, Epsilon: 0.81\n",
      "Episode 41, Reward: 33.0, Epsilon: 0.81\n",
      "Episode 42, Reward: 34.0, Epsilon: 0.81\n",
      "Episode 43, Reward: 17.0, Epsilon: 0.80\n",
      "Episode 44, Reward: 12.0, Epsilon: 0.80\n",
      "Episode 45, Reward: 35.0, Epsilon: 0.79\n",
      "Episode 46, Reward: 92.0, Epsilon: 0.79\n",
      "Episode 47, Reward: 20.0, Epsilon: 0.79\n",
      "Episode 48, Reward: 49.0, Epsilon: 0.78\n",
      "Episode 49, Reward: 120.0, Epsilon: 0.78\n",
      "Episode 50, Reward: 17.0, Epsilon: 0.77\n",
      "Episode 51, Reward: 30.0, Epsilon: 0.77\n",
      "Episode 52, Reward: 30.0, Epsilon: 0.77\n",
      "Episode 53, Reward: 14.0, Epsilon: 0.76\n",
      "Episode 54, Reward: 20.0, Epsilon: 0.76\n",
      "Episode 55, Reward: 16.0, Epsilon: 0.76\n",
      "Episode 56, Reward: 17.0, Epsilon: 0.75\n",
      "Episode 57, Reward: 80.0, Epsilon: 0.75\n",
      "Episode 58, Reward: 59.0, Epsilon: 0.74\n",
      "Episode 59, Reward: 60.0, Epsilon: 0.74\n",
      "Episode 60, Reward: 29.0, Epsilon: 0.74\n",
      "Episode 61, Reward: 32.0, Epsilon: 0.73\n",
      "Episode 62, Reward: 13.0, Epsilon: 0.73\n",
      "Episode 63, Reward: 23.0, Epsilon: 0.73\n",
      "Episode 64, Reward: 12.0, Epsilon: 0.72\n",
      "Episode 65, Reward: 36.0, Epsilon: 0.72\n",
      "Episode 66, Reward: 76.0, Epsilon: 0.71\n",
      "Episode 67, Reward: 67.0, Epsilon: 0.71\n",
      "Episode 68, Reward: 136.0, Epsilon: 0.71\n",
      "Episode 69, Reward: 76.0, Epsilon: 0.70\n",
      "Episode 70, Reward: 22.0, Epsilon: 0.70\n",
      "Episode 71, Reward: 22.0, Epsilon: 0.70\n",
      "Episode 72, Reward: 87.0, Epsilon: 0.69\n",
      "Episode 73, Reward: 39.0, Epsilon: 0.69\n",
      "Episode 74, Reward: 22.0, Epsilon: 0.69\n",
      "Episode 75, Reward: 39.0, Epsilon: 0.68\n",
      "Episode 76, Reward: 22.0, Epsilon: 0.68\n",
      "Episode 77, Reward: 61.0, Epsilon: 0.68\n",
      "Episode 78, Reward: 38.0, Epsilon: 0.67\n",
      "Episode 79, Reward: 32.0, Epsilon: 0.67\n",
      "Episode 80, Reward: 72.0, Epsilon: 0.67\n",
      "Episode 81, Reward: 15.0, Epsilon: 0.66\n",
      "Episode 82, Reward: 74.0, Epsilon: 0.66\n",
      "Episode 83, Reward: 13.0, Epsilon: 0.66\n",
      "Episode 84, Reward: 37.0, Epsilon: 0.65\n",
      "Episode 85, Reward: 102.0, Epsilon: 0.65\n",
      "Episode 86, Reward: 54.0, Epsilon: 0.65\n",
      "Episode 87, Reward: 121.0, Epsilon: 0.64\n",
      "Episode 88, Reward: 21.0, Epsilon: 0.64\n",
      "Episode 89, Reward: 73.0, Epsilon: 0.64\n",
      "Episode 90, Reward: 72.0, Epsilon: 0.63\n",
      "Episode 91, Reward: 38.0, Epsilon: 0.63\n",
      "Episode 92, Reward: 53.0, Epsilon: 0.63\n",
      "Episode 93, Reward: 94.0, Epsilon: 0.62\n",
      "Episode 94, Reward: 82.0, Epsilon: 0.62\n",
      "Episode 95, Reward: 16.0, Epsilon: 0.62\n",
      "Episode 96, Reward: 30.0, Epsilon: 0.61\n",
      "Episode 97, Reward: 80.0, Epsilon: 0.61\n",
      "Episode 98, Reward: 47.0, Epsilon: 0.61\n",
      "Episode 99, Reward: 100.0, Epsilon: 0.61\n",
      "Episode 100, Reward: 28.0, Epsilon: 0.60\n",
      "Episode 101, Reward: 16.0, Epsilon: 0.60\n",
      "Episode 102, Reward: 52.0, Epsilon: 0.60\n",
      "Episode 103, Reward: 45.0, Epsilon: 0.59\n",
      "Episode 104, Reward: 32.0, Epsilon: 0.59\n",
      "Episode 105, Reward: 13.0, Epsilon: 0.59\n",
      "Episode 106, Reward: 147.0, Epsilon: 0.58\n",
      "Episode 107, Reward: 84.0, Epsilon: 0.58\n",
      "Episode 108, Reward: 18.0, Epsilon: 0.58\n",
      "Episode 109, Reward: 42.0, Epsilon: 0.58\n",
      "Episode 110, Reward: 66.0, Epsilon: 0.57\n",
      "Episode 111, Reward: 66.0, Epsilon: 0.57\n",
      "Episode 112, Reward: 24.0, Epsilon: 0.57\n",
      "Episode 113, Reward: 201.0, Epsilon: 0.56\n",
      "Episode 114, Reward: 32.0, Epsilon: 0.56\n",
      "Episode 115, Reward: 47.0, Epsilon: 0.56\n",
      "Episode 116, Reward: 217.0, Epsilon: 0.56\n",
      "Episode 117, Reward: 106.0, Epsilon: 0.55\n",
      "Episode 118, Reward: 76.0, Epsilon: 0.55\n",
      "Episode 119, Reward: 50.0, Epsilon: 0.55\n",
      "Episode 120, Reward: 252.0, Epsilon: 0.55\n",
      "Episode 121, Reward: 32.0, Epsilon: 0.54\n",
      "Episode 122, Reward: 33.0, Epsilon: 0.54\n",
      "Episode 123, Reward: 57.0, Epsilon: 0.54\n",
      "Episode 124, Reward: 134.0, Epsilon: 0.53\n",
      "Episode 125, Reward: 81.0, Epsilon: 0.53\n",
      "Episode 126, Reward: 149.0, Epsilon: 0.53\n",
      "Episode 127, Reward: 145.0, Epsilon: 0.53\n",
      "Episode 128, Reward: 94.0, Epsilon: 0.52\n",
      "Episode 129, Reward: 135.0, Epsilon: 0.52\n",
      "Episode 130, Reward: 24.0, Epsilon: 0.52\n",
      "Episode 131, Reward: 163.0, Epsilon: 0.52\n",
      "Episode 132, Reward: 22.0, Epsilon: 0.51\n",
      "Episode 133, Reward: 60.0, Epsilon: 0.51\n",
      "Episode 134, Reward: 108.0, Epsilon: 0.51\n",
      "Episode 135, Reward: 86.0, Epsilon: 0.51\n",
      "Episode 136, Reward: 120.0, Epsilon: 0.50\n",
      "Episode 137, Reward: 35.0, Epsilon: 0.50\n",
      "Episode 138, Reward: 137.0, Epsilon: 0.50\n",
      "Episode 139, Reward: 261.0, Epsilon: 0.50\n",
      "Episode 140, Reward: 14.0, Epsilon: 0.49\n",
      "Episode 141, Reward: 43.0, Epsilon: 0.49\n",
      "Episode 142, Reward: 164.0, Epsilon: 0.49\n",
      "Episode 143, Reward: 500.0, Epsilon: 0.49\n",
      "Episode 144, Reward: 72.0, Epsilon: 0.48\n",
      "Episode 145, Reward: 447.0, Epsilon: 0.48\n",
      "Episode 146, Reward: 314.0, Epsilon: 0.48\n",
      "Episode 147, Reward: 110.0, Epsilon: 0.48\n",
      "Episode 148, Reward: 17.0, Epsilon: 0.47\n",
      "Episode 149, Reward: 97.0, Epsilon: 0.47\n",
      "Episode 150, Reward: 79.0, Epsilon: 0.47\n",
      "Episode 151, Reward: 76.0, Epsilon: 0.47\n",
      "Episode 152, Reward: 280.0, Epsilon: 0.46\n",
      "Episode 153, Reward: 24.0, Epsilon: 0.46\n",
      "Episode 154, Reward: 251.0, Epsilon: 0.46\n",
      "Episode 155, Reward: 213.0, Epsilon: 0.46\n",
      "Episode 156, Reward: 18.0, Epsilon: 0.46\n",
      "Episode 157, Reward: 65.0, Epsilon: 0.45\n",
      "Episode 158, Reward: 102.0, Epsilon: 0.45\n",
      "Episode 159, Reward: 158.0, Epsilon: 0.45\n",
      "Episode 160, Reward: 113.0, Epsilon: 0.45\n",
      "Episode 161, Reward: 31.0, Epsilon: 0.44\n",
      "Episode 162, Reward: 157.0, Epsilon: 0.44\n",
      "Episode 163, Reward: 22.0, Epsilon: 0.44\n",
      "Episode 164, Reward: 17.0, Epsilon: 0.44\n",
      "Episode 165, Reward: 14.0, Epsilon: 0.44\n",
      "Episode 166, Reward: 129.0, Epsilon: 0.43\n",
      "Episode 167, Reward: 91.0, Epsilon: 0.43\n",
      "Episode 168, Reward: 43.0, Epsilon: 0.43\n",
      "Episode 169, Reward: 115.0, Epsilon: 0.43\n",
      "Episode 170, Reward: 16.0, Epsilon: 0.42\n",
      "Episode 171, Reward: 100.0, Epsilon: 0.42\n",
      "Episode 172, Reward: 108.0, Epsilon: 0.42\n",
      "Episode 173, Reward: 19.0, Epsilon: 0.42\n",
      "Episode 174, Reward: 133.0, Epsilon: 0.42\n",
      "Episode 175, Reward: 75.0, Epsilon: 0.41\n",
      "Episode 176, Reward: 34.0, Epsilon: 0.41\n",
      "Episode 177, Reward: 56.0, Epsilon: 0.41\n",
      "Episode 178, Reward: 58.0, Epsilon: 0.41\n",
      "Episode 179, Reward: 19.0, Epsilon: 0.41\n",
      "Episode 180, Reward: 69.0, Epsilon: 0.40\n",
      "Episode 181, Reward: 16.0, Epsilon: 0.40\n",
      "Episode 182, Reward: 35.0, Epsilon: 0.40\n",
      "Episode 183, Reward: 60.0, Epsilon: 0.40\n",
      "Episode 184, Reward: 72.0, Epsilon: 0.40\n",
      "Episode 185, Reward: 202.0, Epsilon: 0.39\n",
      "Episode 186, Reward: 175.0, Epsilon: 0.39\n",
      "Episode 187, Reward: 127.0, Epsilon: 0.39\n",
      "Episode 188, Reward: 133.0, Epsilon: 0.39\n",
      "Episode 189, Reward: 171.0, Epsilon: 0.39\n",
      "Episode 190, Reward: 14.0, Epsilon: 0.38\n",
      "Episode 191, Reward: 127.0, Epsilon: 0.38\n",
      "Episode 192, Reward: 123.0, Epsilon: 0.38\n",
      "Episode 193, Reward: 93.0, Epsilon: 0.38\n",
      "Episode 194, Reward: 14.0, Epsilon: 0.38\n",
      "Episode 195, Reward: 17.0, Epsilon: 0.37\n",
      "Episode 196, Reward: 149.0, Epsilon: 0.37\n",
      "Episode 197, Reward: 23.0, Epsilon: 0.37\n",
      "Episode 198, Reward: 122.0, Epsilon: 0.37\n",
      "Episode 199, Reward: 137.0, Epsilon: 0.37\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.q_net = DQN(input_dim, hidden_dim, output_dim)\n",
    "        self.target_net = DQN(input_dim, hidden_dim, output_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.output_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "        target = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            \n",
    "# ...existing code...\n",
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64\n",
    "agent = DQNAgent(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "episodes = 200\n",
    "batch_size = 64\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(500):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.update(batch_size)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    agent.update_target()\n",
    "    print(f\"Episode {ep}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8891ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')    \n",
    "state, _ = env.reset()  # 处理Gym v0.26+的返回值\n",
    "frame = env.render()\n",
    "plt.ion()  # 打开交互模式\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(frame)\n",
    "display.display(plt.gcf())\n",
    "\n",
    "for _ in range(512):\n",
    "    try:\n",
    "        frame = env.render()\n",
    "    except Exception as e:\n",
    "        print(f\"渲染错误: {e}\")\n",
    "        break\n",
    "    img.set_data(frame)  # 只更新图像数据，而不是重建\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    action = agent.get_action(state)\n",
    "    # if state[2] < 0:\n",
    "    #     action = 0\n",
    "    # else:\n",
    "    #     action = 1\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)  # Gym v0.26+的返回参数\n",
    "    state = next_state\n",
    "    if  truncated:\n",
    "        state, _ = env.reset()  # 重置环境并获取初始状态\n",
    "plt.ioff()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

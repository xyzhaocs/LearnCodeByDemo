{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "d = envs.registry\n",
    "for k, v in d.items():\n",
    "    print(f\"{k} -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')      \n",
    "obs, _ = env.reset()\n",
    "\n",
    "# 初始化图像对象\n",
    "frame = env.render()\n",
    "plt.ion()  # 打开交互模式\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(frame)\n",
    "display.display(plt.gcf())\n",
    "\n",
    "for _ in range(500):\n",
    "    frame = env.render()\n",
    "    img.set_data(frame)  # 只更新图像数据，而不是重建\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    # plt.pause(0.001)  # 小延时允许刷新\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Action taken: {action} {env.observation_space}\")\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    # if terminated or truncated:\n",
    "        # print(f\"{terminated} {truncated}\")\n",
    "        # obs, _ = env.reset()\n",
    "\n",
    "plt.ioff()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "# 定义策略网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# 定义价值网络\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# PPO 代理\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr_policy=3e-4, lr_value=1e-3, gamma=0.99, lambd=0.95, epsilon=0.2):\n",
    "        self.policy = PolicyNetwork(input_dim, hidden_dim, output_dim)\n",
    "        self.value = ValueNetwork(input_dim, hidden_dim)\n",
    "    \n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr_value)\n",
    "    \n",
    "        self.gamma = gamma\n",
    "        self.lambd = lambd\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.policy(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def compute_advantages(self, rewards, values, next_values, dones):\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        advantages[-1] = rewards[-1] + self.gamma * (1 - dones[-1]) * next_values[-1] - values[-1]\n",
    "    \n",
    "        for t in reversed(range(len(rewards)-1)):\n",
    "            delta = rewards[t] + self.gamma * (1 - dones[t]) * next_values[t] - values[t]\n",
    "            advantages[t] = delta + self.gamma * self.lambd * (1 - dones[t]) * advantages[t+1]\n",
    "    \n",
    "        returns = advantages + values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, states, actions, log_probs_old, returns, advantages):\n",
    "        states = torch.tensor(states, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(-1)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        returns = torch.tensor(returns, dtype=torch.float).unsqueeze(-1)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float).unsqueeze(-1)\n",
    "    \n",
    "        for _ in range(10):  # 更新纪元数\n",
    "            log_probs = self.policy(states).gather(1, actions)\n",
    "            ratio = (log_probs / log_probs_old).exp()\n",
    "        \n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "        \n",
    "            loss_policy = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "            self.optimizer_policy.zero_grad()\n",
    "            loss_policy.backward()\n",
    "            self.optimizer_policy.step()\n",
    "        \n",
    "            values = self.value(states)\n",
    "            loss_value = nn.MSELoss()(values, returns)\n",
    "        \n",
    "            self.optimizer_value.zero_grad()\n",
    "            loss_value.backward()\n",
    "            self.optimizer_value.step()\n",
    "\n",
    "def train_ppo(env, agent, max_timesteps=500, batch_size=5000, n_updates=10):\n",
    "    state, _ = env.reset()  # 处理Gym v0.26+的返回值\n",
    "    episode_rewards = []\n",
    "    all_rewards = []\n",
    "    t = 0\n",
    "\n",
    "    while t < max_timesteps:\n",
    "        states, actions, rewards, next_states, dones, log_probs = [], [], [], [], [], []\n",
    "        batch_rewards = 0\n",
    "    \n",
    "        # 收集轨迹\n",
    "        for _ in range(batch_size):\n",
    "            action, log_prob = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # Gym v0.26+的返回参数\n",
    "            done = terminated or truncated  # 合并终止标志\n",
    "        \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            log_probs.append(log_prob)\n",
    "        \n",
    "            if done:\n",
    "                state, _ = env.reset()  # 重置环境并获取初始状态\n",
    "            else:\n",
    "                state = next_state\n",
    "            batch_rewards += reward\n",
    "            t += 1\n",
    "        \n",
    "            if done or t >= max_timesteps:\n",
    "                break\n",
    "    \n",
    "        episode_rewards.append(batch_rewards)\n",
    "        all_rewards.extend(rewards)\n",
    "    \n",
    "        # 计算回报和优势\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float)\n",
    "        next_states_tensor = torch.tensor(np.array(next_states), dtype=torch.float)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float).unsqueeze(-1)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            values = agent.value(states_tensor).squeeze()\n",
    "            next_values = agent.value(next_states_tensor).squeeze()\n",
    "    \n",
    "        returns, advantages = agent.compute_advantages(torch.tensor(rewards, dtype=torch.float), \n",
    "                                                       values, next_values, dones_tensor.squeeze())\n",
    "    \n",
    "        # 更新策略和价值网络\n",
    "        agent.update(states, actions, log_probs, returns.numpy(), advantages.numpy())\n",
    "    \n",
    "        if t >= max_timesteps:\n",
    "            break\n",
    "\n",
    "    return all_rewards, episode_rewards\n",
    "\n",
    "# 初始化环境\n",
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64\n",
    "\n",
    "# 初始化PPO代理\n",
    "agent = PPOAgent(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 训练代理\n",
    "max_timesteps = 1000\n",
    "batch_size = 2048\n",
    "all_rewards, episode_rewards = train_ppo(env, agent, max_timesteps, batch_size)\n",
    "\n",
    "# 打印每集的平均奖励\n",
    "print(\"每集的平均奖励:\", np.mean(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63562593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')    \n",
    "state, _ = env.reset()  # 处理Gym v0.26+的返回值\n",
    "frame = env.render()\n",
    "plt.ion()  # 打开交互模式\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(frame)\n",
    "display.display(plt.gcf())\n",
    "\n",
    "for _ in range(batch_size):\n",
    "    try:\n",
    "        frame = env.render()\n",
    "    except Exception as e:\n",
    "        print(f\"渲染错误: {e}\")\n",
    "        break\n",
    "    img.set_data(frame)  # 只更新图像数据，而不是重建\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    action, log_prob = agent.get_action(state)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)  # Gym v0.26+的返回参数\n",
    "    state = next_state\n",
    "    if  truncated:\n",
    "        state, _ = env.reset()  # 重置环境并获取初始状态\n",
    "plt.ioff()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
